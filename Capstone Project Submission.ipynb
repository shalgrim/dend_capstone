{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2016 Immigration Data Model\n",
    "\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "I executed the Udacity provided project. I used the immigration dataset and city demographics dataset. I stored my data in a star schema in a Postgres database. I used that star schema to answer questions about where (i.e., what countries) immigrants were coming from and where in the U.S. they were headed to.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the immigration data has 28 columns\n",
    "pd.set_option('display.max_columns', 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "\n",
    "I used the immigration dataset and city demographics datasets provided by Udacity. I used pandas to read the data and load it into Postgres. From there I did all my ETL and analysis in SQL, but I have provided Python scripts to make it replicable.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "\n",
    "I split the immigration dataset up into a single `fact_immmigration` table as well as several `dim_` dimension tables. The city demographics dataset resulted in one intial dimension table that was the raw data and then I aggregated it into state-level statistics in another dimention table.\n",
    "\n",
    "Before loading the data into SQL, I did some exploratory data analysis in pandas to get an idea of what DDL should define my tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "READ_CHUNK_SIZE = 500000\n",
    "IMMIGRATION_DATA_FILENAMES = [\n",
    "    'data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat',\n",
    "    'data/18-83510-I94-Data-2016/i94_feb16_sub.sas7bdat',\n",
    "    'data/18-83510-I94-Data-2016/i94_mar16_sub.sas7bdat',\n",
    "    'data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat',\n",
    "    'data/18-83510-I94-Data-2016/i94_may16_sub.sas7bdat',\n",
    "    'data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat',\n",
    "    'data/18-83510-I94-Data-2016/i94_jul16_sub.sas7bdat',\n",
    "    'data/18-83510-I94-Data-2016/i94_aug16_sub.sas7bdat',\n",
    "    'data/18-83510-I94-Data-2016/i94_sep16_sub.sas7bdat',\n",
    "    'data/18-83510-I94-Data-2016/i94_oct16_sub.sas7bdat',\n",
    "    'data/18-83510-I94-Data-2016/i94_nov16_sub.sas7bdat',\n",
    "    'data/18-83510-I94-Data-2016/i94_dec16_sub.sas7bdat',\n",
    "]\n",
    "CONN_STRING = 'postgresql://capstone_user:capstone_pw@localhost:5432/capstone'\n",
    "HEADER_FILE = 'data/I94_SAS_Labels_Descriptions.SAS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes several hours to load all of the immigration data into a single dataframe, so for the initial EDA I just loaded up one chunk of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = IMMIGRATION_DATA_FILENAMES[0]\n",
    "iterator = pd.read_sas(\n",
    "    filename, 'sas7bdat', encoding='ISO-8859-1', chunksize=READ_CHUNK_SIZE\n",
    ")\n",
    "immigration_df = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500000, 28)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>count</th>\n",
       "      <th>dtadfile</th>\n",
       "      <th>visapost</th>\n",
       "      <th>occup</th>\n",
       "      <th>entdepa</th>\n",
       "      <th>entdepd</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20465.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LH</td>\n",
       "      <td>346608285.0</td>\n",
       "      <td>424</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20465.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LH</td>\n",
       "      <td>346627585.0</td>\n",
       "      <td>424</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20469.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CT</td>\n",
       "      <td>20480.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>07152016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AF</td>\n",
       "      <td>381092385.0</td>\n",
       "      <td>338</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20469.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CT</td>\n",
       "      <td>20499.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1971.0</td>\n",
       "      <td>07152016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AF</td>\n",
       "      <td>381087885.0</td>\n",
       "      <td>338</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20469.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CT</td>\n",
       "      <td>20499.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>07152016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AF</td>\n",
       "      <td>381078685.0</td>\n",
       "      <td>338</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    7.0  2016.0     1.0   101.0   101.0     BOS  20465.0      1.0      MA   \n",
       "1    8.0  2016.0     1.0   101.0   101.0     BOS  20465.0      1.0      MA   \n",
       "2    9.0  2016.0     1.0   101.0   101.0     BOS  20469.0      1.0      CT   \n",
       "3   10.0  2016.0     1.0   101.0   101.0     BOS  20469.0      1.0      CT   \n",
       "4   11.0  2016.0     1.0   101.0   101.0     BOS  20469.0      1.0      CT   \n",
       "\n",
       "   depdate  i94bir  i94visa  count dtadfile visapost occup entdepa entdepd  \\\n",
       "0      NaN    20.0      3.0    1.0      NaN      NaN   NaN       T     NaN   \n",
       "1      NaN    20.0      3.0    1.0      NaN      NaN   NaN       T     NaN   \n",
       "2  20480.0    17.0      2.0    1.0      NaN      NaN   NaN       T       N   \n",
       "3  20499.0    45.0      2.0    1.0      NaN      NaN   NaN       T       N   \n",
       "4  20499.0    12.0      2.0    1.0      NaN      NaN   NaN       T       N   \n",
       "\n",
       "  entdepu matflag  biryear   dtaddto gender insnum airline       admnum fltno  \\\n",
       "0     NaN     NaN   1996.0       D/S      M    NaN      LH  346608285.0   424   \n",
       "1     NaN     NaN   1996.0       D/S      M    NaN      LH  346627585.0   424   \n",
       "2     NaN       M   1999.0  07152016      F    NaN      AF  381092385.0   338   \n",
       "3     NaN       M   1971.0  07152016      F    NaN      AF  381087885.0   338   \n",
       "4     NaN       M   2004.0  07152016      M    NaN      AF  381078685.0   338   \n",
       "\n",
       "  visatype  \n",
       "0       F1  \n",
       "1       F1  \n",
       "2       B2  \n",
       "3       B2  \n",
       "4       B2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(immigration_df.shape)\n",
    "immigration_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is only 500,000 rows out of a 40.79 million row dataset. Because it takes hours to load all of that data, and because `pd.to_sql` does not work from an ipython kernel, the full data load happens in Python scripts documented below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The city demographics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2891, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_demo_df = pd.read_csv('data/us-cities-demographics.csv', delimiter=';')\n",
    "print(city_demo_df.shape)\n",
    "city_demo_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "\n",
    "#### Immigration Data\n",
    "\n",
    "Because this data was too big to load into a single dataframe and do exploration on, all of the data exploration, assessment, and cleaning came after it was loaded into Postgres. My SQL exploration steps are detailed in the `Data Quality Checks` section below.\n",
    "\n",
    "I can say, though, that most of my cleaning of this data involved creating dim tables for many of the coded fields, like `i94res` `i94cit`, and `i94visa`. Any data in those columns that didn't match a key in those dim tables would be interpreted as \"unknown.\"\n",
    "\n",
    "I did find two columns that could be cleaned. The first is `i94bir`, which is the age of the respondent in years. In this case I will set anything less than zero and the two 1812 values to NULL. `biryear` is the reported birth year of entrants and most of its dates range from 1900 - 2016, but there are two instances of 204, two of 2018, and one from 2019. Since this data is from 2016 those cases will all get set to NULL.\n",
    "\n",
    "I considered cleaning columns like `i94addr` where the values provided go outside the bounds of what's listed in the header file and thus my dimension tables created from that table. However, some of these columns (`i94port`, `i94cit`, and `i94res`) provide more than one \"invalid\" code that it wasn't clear what to set it to. Another, `i94addr` provides just one `All Other Codes` code, but the value in that column, even that outside of the valid values, was so rich I didn't want to get rid of it. Consider:\n",
    "\n",
    "```sql\n",
    "select i94addr, count(i94addr) from fact_immigration\n",
    "where i94addr not in (select code from dim_address)\n",
    "group by i94addr order by count desc;\n",
    "```\n",
    "\n",
    "gives\n",
    "\n",
    "| code | count  |\n",
    "| ---- | ------:|\n",
    "| MP   | 130555 |\n",
    "| US   | 108767 |\n",
    "| VQ   | 49465  |\n",
    "| UN   | 20383  |\n",
    "| GQ   | 10998  |\n",
    "\n",
    "Finally, since this data also involves 12 different files, we need to make sure that the columns in the data are consistent across each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_values([['jan', 'feb', 'mar', 'apr', 'may', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec'], ['jun']])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring immigration data columns across all the files\n",
    "\n",
    "# Put a small dataframe from each month into a list\n",
    "dfs = []\n",
    "for fname in IMMIGRATION_DATA_FILENAMES:\n",
    "    myiter = pd.read_sas(fname, 'sas7bdat', encoding='ISO-8859-1', chunksize=20)\n",
    "    dfs.append(next(myiter))\n",
    "    \n",
    "# create a dict that maps each month to a list of column names\n",
    "cnames_by_fname = {t[0].split('/')[-1].split('_')[1][:3]: list(t[1].columns.values)\n",
    "                   for t in zip(IMMIGRATION_DATA_FILENAMES, dfs)}\n",
    "\n",
    "# let's hope for some commonality and reverse that dict so a hashed up comma-seperated\n",
    "# list of the column names is the key and the values are lists of the months\n",
    "cbyf_reversed = defaultdict(list)\n",
    "for k, v in cnames_by_fname.items():\n",
    "    cbyf_reversed[','.join(v)].append(k)\n",
    "    \n",
    "print(len(cbyf_reversed))\n",
    "cbyf_reversed.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 34\n",
      "('cicid', 'cicid')\n",
      "('i94yr', 'i94yr')\n",
      "('i94mon', 'i94mon')\n",
      "('i94cit', 'i94cit')\n",
      "('i94res', 'i94res')\n",
      "('i94port', 'i94port')\n",
      "('arrdate', 'arrdate')\n",
      "('i94mode', 'i94mode')\n",
      "('i94addr', 'i94addr')\n",
      "('depdate', 'depdate')\n",
      "('i94bir', 'i94bir')\n",
      "('i94visa', 'i94visa')\n",
      "('count', 'count')\n",
      "('dtadfile', 'validres')\n",
      "('visapost', 'delete_days')\n",
      "('occup', 'delete_mexl')\n",
      "('entdepa', 'delete_dup')\n",
      "('entdepd', 'delete_visa')\n",
      "('entdepu', 'delete_recdup')\n",
      "('matflag', 'dtadfile')\n",
      "('biryear', 'visapost')\n",
      "('dtaddto', 'occup')\n",
      "('gender', 'entdepa')\n",
      "('insnum', 'entdepd')\n",
      "('airline', 'entdepu')\n",
      "('admnum', 'matflag')\n",
      "('fltno', 'biryear')\n",
      "('visatype', 'dtaddto')\n"
     ]
    }
   ],
   "source": [
    "# We can see that June is the odd duck out, let's take a closer look at how these columns\n",
    "# do and don't match up then\n",
    "\n",
    "usual = list(dfs[0].columns.values)\n",
    "gray_duck = list(dfs[5].columns.values)\n",
    "print(len(usual), len(gray_duck))\n",
    "for z in (zip(usual, gray_duck)):\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# My eyes say that June just has a `validres` and then five `delete_` columns added\n",
    "# to the middle. Let's see if we can validate that\n",
    "\n",
    "usual == [gd for gd in gray_duck if not (gd.startswith('delete_') or gd == 'validres')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when we insert this immigration data we'll have to take special care with the June data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### City Demographics Data\n",
    "\n",
    "This data looks pretty clean. The exploration steps involve:\n",
    "\n",
    "1. Looking at a single city. This reveals the grain of the table to be city/state/race. Removing the `Race` and `Count` columns gives you duplicate data\n",
    "1. Assuring that the `Male Population` and `Female Population` numbers add up to `Total Population` whenever they are present. (NB: I am aware not everybody identifies as exactly one of those two genders, but they always add up to total in this table so I assume those were the only two choices and were required.)\n",
    "1. Assuring that `Number of Veterans`, `Race`, and `Foreign-born` are less than `Total Population` whenever they are present.\n",
    "1. Seeing that the total of `Count` for all the races adds up to more than `Total Population`, indicating people must have been allowed to select more than one race for themselves in the survey.\n",
    "1. Selecting distinct state codes to be sure that the number was near 50 (to account for 50 states, DC, PR, etc.)\n",
    "\n",
    "Those are important things to learn, but there was nothing in this data that needed cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>MD</td>\n",
       "      <td>White</td>\n",
       "      <td>37756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1678</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>MD</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>21330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2123</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>MD</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>1084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2162</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>MD</td>\n",
       "      <td>Asian</td>\n",
       "      <td>8841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City     State  Median Age  Male Population  Female Population  \\\n",
       "0     Silver Spring  Maryland        33.8          40601.0            41862.0   \n",
       "592   Silver Spring  Maryland        33.8          40601.0            41862.0   \n",
       "1678  Silver Spring  Maryland        33.8          40601.0            41862.0   \n",
       "2123  Silver Spring  Maryland        33.8          40601.0            41862.0   \n",
       "2162  Silver Spring  Maryland        33.8          40601.0            41862.0   \n",
       "\n",
       "      Total Population  Number of Veterans  Foreign-born  \\\n",
       "0                82463              1562.0       30908.0   \n",
       "592              82463              1562.0       30908.0   \n",
       "1678             82463              1562.0       30908.0   \n",
       "2123             82463              1562.0       30908.0   \n",
       "2162             82463              1562.0       30908.0   \n",
       "\n",
       "      Average Household Size State Code                               Race  \\\n",
       "0                        2.6         MD                 Hispanic or Latino   \n",
       "592                      2.6         MD                              White   \n",
       "1678                     2.6         MD          Black or African-American   \n",
       "2123                     2.6         MD  American Indian and Alaska Native   \n",
       "2162                     2.6         MD                              Asian   \n",
       "\n",
       "      Count  \n",
       "0     25924  \n",
       "592   37756  \n",
       "1678  21330  \n",
       "2123   1084  \n",
       "2162   8841  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring city demographics data\n",
    "\n",
    "# Looking at one city reveals grain to be city/state/race\n",
    "city_demo_df[city_demo_df['City'] == 'Silver Spring'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abilene</td>\n",
       "      <td>Texas</td>\n",
       "      <td>125876</td>\n",
       "      <td>147900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Akron</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>197553</td>\n",
       "      <td>210305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alafaya</td>\n",
       "      <td>Florida</td>\n",
       "      <td>85264</td>\n",
       "      <td>115476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alameda</td>\n",
       "      <td>California</td>\n",
       "      <td>78614</td>\n",
       "      <td>89174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Albany</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>71109</td>\n",
       "      <td>73478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      City       State  Total Population   Count\n",
       "0  Abilene       Texas            125876  147900\n",
       "1    Akron        Ohio            197553  210305\n",
       "2  Alafaya     Florida             85264  115476\n",
       "3  Alameda  California             78614   89174\n",
       "4   Albany     Georgia             71109   73478"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# More city demographics exploration\n",
    "\n",
    "# Ensure counts, when present, don't exceed total population, and that male+female == total\n",
    "for _, row in city_demo_df.iterrows():\n",
    "    if pd.notnull(row['Male Population']):\n",
    "        assert row['Male Population'] + row['Female Population'] == row['Total Population']\n",
    "    if pd.notnull(row['Number of Veterans']):\n",
    "        assert row['Number of Veterans'] <= row['Total Population']\n",
    "    if pd.notnull(row['Foreign-born']):\n",
    "        assert row['Foreign-born'] <= row['Total Population']\n",
    "    if pd.notnull(row['Count']):\n",
    "        assert row['Count'] <= row['Total Population']\n",
    "    for numbered_column in ['Male Population', 'Female Population', 'Total Population', 'Number of Veterans', 'Foreign-born', 'Count']:\n",
    "        assert pd.isnull(row[numbered_column]) or row[numbered_column] > 0\n",
    "        \n",
    "# Check to see if race counts add up to total\n",
    "sub_df = city_demo_df[['City', 'State', 'Total Population', 'Count']]\n",
    "grouped = sub_df.groupby(['City', 'State', 'Total Population']).sum().reset_index()\n",
    "grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayamón Puerto Rico 170259 169155\n",
      "Caguas Puerto Rico 77008 76973\n",
      "Mayagüez Puerto Rico 66581 65756\n",
      "New Bedford Massachusetts 94959 93321\n",
      "Ponce Puerto Rico 121583 120705\n",
      "San Juan Puerto Rico 342237 342042\n",
      "South Jordan Utah 66639 66205\n"
     ]
    }
   ],
   "source": [
    "# More city demographics exploration\n",
    "\n",
    "# Based on that last run it looks like Count always sums up to >= Total Population, so let's check\n",
    "for _, row in grouped.iterrows():\n",
    "    if pd.notnull(row['Total Population']) and row['Count'] < row['Total Population']:\n",
    "        print(row['City'], row['State'], row['Total Population'], row['Count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's interesting. Five from Puerto Rico, one from Massachusetts, and another from Utah. Not entirely unsurprising, as those are all places where you would expect a lack of diversity. The numbers are close, and while this leads us to conclude `Race` may have been optional or there may be missing counts, I don't know if we can do anything to clean this.\n",
    "\n",
    "Let's look at states now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State Code</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Foreign-born</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA</td>\n",
       "      <td>61055672.0</td>\n",
       "      <td>62388681.0</td>\n",
       "      <td>123444353</td>\n",
       "      <td>37059662.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>TX</td>\n",
       "      <td>34862194.0</td>\n",
       "      <td>35691659.0</td>\n",
       "      <td>70553853</td>\n",
       "      <td>14498054.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>NY</td>\n",
       "      <td>23422799.0</td>\n",
       "      <td>25579256.0</td>\n",
       "      <td>49002055</td>\n",
       "      <td>17186873.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FL</td>\n",
       "      <td>15461937.0</td>\n",
       "      <td>16626425.0</td>\n",
       "      <td>32306132</td>\n",
       "      <td>7845566.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>IL</td>\n",
       "      <td>10943864.0</td>\n",
       "      <td>11570526.0</td>\n",
       "      <td>22514390</td>\n",
       "      <td>4632600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   State Code  Male Population  Female Population  Total Population  \\\n",
       "4          CA       61055672.0         62388681.0         123444353   \n",
       "44         TX       34862194.0         35691659.0          70553853   \n",
       "34         NY       23422799.0         25579256.0          49002055   \n",
       "9          FL       15461937.0         16626425.0          32306132   \n",
       "14         IL       10943864.0         11570526.0          22514390   \n",
       "\n",
       "    Foreign-born  \n",
       "4     37059662.0  \n",
       "44    14498054.0  \n",
       "34    17186873.0  \n",
       "9      7845566.0  \n",
       "14     4632600.0  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# More city demographics exploration\n",
    "\n",
    "# Let's get some totals by state now, in large part to verify that we only have ~ 50 states listed\n",
    "sub_df = city_demo_df[['State Code', 'Male Population', 'Female Population', 'Total Population', 'Foreign-born']]\n",
    "summed_by_state_df = sub_df.groupby(['State Code']).sum().reset_index()\n",
    "print(summed_by_state_df.shape)\n",
    "summed_by_state_df.sort_values(by=['Total Population'], ascending=False).head()  # reveals 49 rows, which is great, and topped by big states like CA, TX, and NY passes sniff test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "I wanted to know more about immigration events, so I put the immigration data at the center of my star schema in a table called `fact_immigration`. This is primarily that data loaded into a table, though I do some data validation, exploration, and cleaning steps after it's loaded, as you'll see below.\n",
    "\n",
    "```sql\n",
    "create table fact_immigration\n",
    "(\n",
    "\timmigration_id serial not null\n",
    "\t\tconstraint fact_immigration_pkey\n",
    "\t\t\tprimary key,\n",
    "\tcicid integer not null,\n",
    "\ti94yr integer not null,\n",
    "\ti94mon integer not null,\n",
    "\ti94cit integer,\n",
    "\ti94res integer,\n",
    "\ti94port char(3),\n",
    "\tarrdate integer,\n",
    "\ti94mode integer,\n",
    "\ti94addr char(3),\n",
    "\tdepdate integer,\n",
    "\ti94bir integer,\n",
    "\ti94visa integer,\n",
    "\tcount integer,\n",
    "\tdtadfile varchar,\n",
    "\tvisapost char(3),\n",
    "\toccup char(3),\n",
    "\tentdepa char,\n",
    "\tentdepd char,\n",
    "\tentdepu char,\n",
    "\tmatflag char,\n",
    "\tbiryear integer,\n",
    "\tdtaddto integer,\n",
    "\tgender char,\n",
    "\tinsnum integer,\n",
    "\tairline char(2),\n",
    "\tadmnum integer,\n",
    "\tfltno varchar,\n",
    "\tvisatype char(2)\n",
    ");\n",
    "```\n",
    "\n",
    "With the help of the immigration data header file, I was able to create several dimension tables around it:\n",
    "\n",
    "- dim_country\n",
    "- dim_arrival_mode\n",
    "- dim_port\n",
    "- dim_address\n",
    "- dim_visa_type\n",
    "\n",
    "##### dim_country\n",
    "\n",
    "The foreign keys are `fact_immigration.i94cit` and `fact_immigration.i94res`\n",
    "\n",
    "```sql\n",
    "create table dim_country\n",
    "(\n",
    "\tcode integer not null\n",
    "\t\tconstraint dim_country_pkey\n",
    "\t\t\tprimary key,\n",
    "\tname varchar not null\n",
    ");\n",
    "```\n",
    "\n",
    "##### dim_arrival_mode\n",
    "\n",
    "The foreign key is `fact_immigration.i94mode`\n",
    "\n",
    "```sql\n",
    "create table dim_arrival_mode\n",
    "(\n",
    "\tcode integer,\n",
    "\tmode char(12)\n",
    ");\n",
    "```\n",
    "\n",
    "##### dim_port\n",
    "\n",
    "The foreign key is `fact_immigration.i94port`\n",
    "\n",
    "```sql\n",
    "create table dim_port\n",
    "(\n",
    "\tcode char(3),\n",
    "\tname varchar\n",
    ");\n",
    "```\n",
    "\n",
    "##### dim_address\n",
    "\n",
    "The foreign key is `fact_immigration.i94addr`\n",
    "\n",
    "```sql\n",
    "create table dim_address\n",
    "(\n",
    "\tcode char(2),\n",
    "\tname varchar\n",
    ");\n",
    "```\n",
    "\n",
    "##### dim_visa_type\n",
    "\n",
    "The foreign key is `fact_immigration.i94visa`\n",
    "\n",
    "```sql\n",
    "create table dim_visa_type\n",
    "(\n",
    "\tcode integer,\n",
    "\tvisa_type char(8)\n",
    ");\n",
    "```\n",
    "\n",
    "##### dim_date\n",
    "\n",
    "Because dates are stored in a few columns in `fact_immigration` and in different formats, I wanted to create a `dim_date` table that covered the entire 20th century (for the older immigrants) and went all the way through to the end of this year.\n",
    "\n",
    "The foreign keys are `arrdate`, `depdate`, `dtadfile`, and `dtaddto` in `fact_immigration`\n",
    "\n",
    "```sql\n",
    "create table dim_date\n",
    "(\n",
    "\tcode integer not null\n",
    "\t\tconstraint dim_date_pkey\n",
    "\t\t\tprimary key,\n",
    "\tyear integer not null,\n",
    "\tmonth integer not null,\n",
    "\tday integer not null,\n",
    "\tday_of_week integer not null,\n",
    "\tymd_dash char(10) not null,\n",
    "\tymd_nodash char(8) not null,\n",
    "\tmdy_nodash char(8) not null\n",
    ");\n",
    "```\n",
    "\n",
    "##### dim_city and dim_state\n",
    "\n",
    "I also used the city demographics data to create two additional tables: `dim_city` and `dim_state`. `dim_city` is essentially just the dataset itself loaded into a table, whereas `dim_state` is the aggregation I came up with in my exploration above.\n",
    "\n",
    "```sql\n",
    "create table dim_city\n",
    "(\n",
    "\tcity varchar,\n",
    "\tstate varchar,\n",
    "\tmedian_age numeric,\n",
    "\tmale_pop integer,\n",
    "\tfemale_pop integer,\n",
    "\ttotal_pop integer,\n",
    "\tnum_vets integer,\n",
    "\tforeign_born integer,\n",
    "\tavg_household_size double precision,\n",
    "\tstate_code char(2),\n",
    "\trace varchar,\n",
    "\tcount integer\n",
    ");\n",
    "\n",
    "create table dim_state\n",
    "(\n",
    "\tstate_code char(2)\n",
    "\t\tconstraint dim_state_pkey\n",
    "\t\t\tprimary key,\n",
    "\tmale_pop integer,\n",
    "\tfemale_pop integer,\n",
    "\ttotal_pop integer,\n",
    "\tforeign_born integer\n",
    ");\n",
    "```\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "I'll start with the easiest tables and build up from there\n",
    "\n",
    "##### dim_arrival_mode and dim_visa_type\n",
    "\n",
    "These two tables have three and four rows in them, respectively. There's no reason to pipeline anything, they can be generated by manually creating simple `INSERT` statements.\n",
    "\n",
    "##### dim_address, dim_port, and dim_country\n",
    "\n",
    "These can be generated by:\n",
    "\n",
    "1. Inspecting the line numbers where their details are listed in the header file\n",
    "1. Crafting regular expressions\n",
    "1. Writing a script to parse values from those lines and insert the values\n",
    "\n",
    "##### dim_city and dim_state\n",
    "\n",
    "The data frames for these tables are available from Step 2 above. They are ready to be inserted into the database\n",
    "\n",
    "##### fact_immigration\n",
    "\n",
    "This is the hardest one to get in and is also the only one that requires to step outside the notebook in order to import.\n",
    "\n",
    "This involves reading in each of the 12 files into a dataframe and then inserting each dataframe into the table. After each insert we delete the dataframe from memory before reading in the next file. The reason for this is that continuing to concatenate each month's worth of data into a single dataframe takes far longer than it does to complete the cycle one month at a time.\n",
    "\n",
    "One exception to the \"one month at a time\" rule is that we do May and June in the same go. The reason for this is, as shown above, June has extra columns in it that we don't want, and we can get rid of those trivially by doing an inner concatenation with May, the month prior to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "\n",
    "#### 4.1 Create the data model\n",
    "\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database connections\n",
    "conn = psycopg2.connect(CONN_STRING)\n",
    "conn.set_session(autocommit=True)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dim_arrival_mode and dim_visa_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_arr_mode_drop = 'DROP TABLE IF EXISTS dim_arrival_mode;'\n",
    "\n",
    "dim_arr_mode_create = \"\"\"CREATE TABLE IF NOT EXISTS dim_arrival_mode\n",
    "(code int, mode char(12))\"\"\"\n",
    "\n",
    "dim_arr_mode_insert = \"\"\"INSERT INTO dim_arrival_mode (code, mode)\n",
    "VALUES (1, 'Air'), (2, 'Sea'), (3, 'Land'), (9, 'Not reported');\"\"\"\n",
    "\n",
    "cur.execute(dim_arr_mode_drop)\n",
    "cur.execute(dim_arr_mode_create)\n",
    "cur.execute(dim_arr_mode_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_visa_type_drop = 'DROP TABLE IF EXISTS dim_visa_type;'\n",
    "\n",
    "dim_visa_type_create = \"\"\"CREATE TABLE IF NOT EXISTS dim_visa_type\n",
    "(code int, visa_type char(8))\"\"\"\n",
    "\n",
    "dim_visa_type_insert = \"\"\"INSERT INTO dim_visa_type (code, visa_type)\n",
    "VALUES (1, 'Business'), (2, 'Pleasure'), (3, 'Student');\"\"\"\n",
    "\n",
    "cur.execute(dim_visa_type_drop)\n",
    "cur.execute(dim_visa_type_create)\n",
    "cur.execute(dim_visa_type_insert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dim_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CODE                 DESCRIPTION\n",
      "I94YR                4 digit year\n",
      "I94MON               Numeric month\n",
      "I94CIT & I94RES      This format shows all the valid and invalid codes for processing\n",
      "I94PORT              This format shows all the valid and invalid codes for processing\n",
      "I94MODE              There are missing values as well as not reported (9)\n",
      "I94BIR               Age of Respondent in Years\n",
      "COUNT                Used for summary statistics\n",
      "DTADFILE             Character Date Field - Date added to I-94 Files - CIC does not use\n",
      "VISAPOST             Department of State where where Visa was issued - CIC does not use\n",
      "OCCUP                Occupation that will be performed in U.S. - CIC does not use\n",
      "ENTDEPA              Arrival Flag - admitted or paroled into the U.S. - CIC does not use\n",
      "ENTDEPD              Departure Flag - Departed, lost I-94 or is deceased - CIC does not use\n",
      "ENTDEPU              Update Flag - Either apprehended, overstayed, adjusted to perm residence - CIC does not use\n",
      "MATFLAG              Match flag - Match of arrival and departure records\n",
      "BIRYEAR              4 digit year of birth\n",
      "DTADDTO              Character Date Field - Date to which admitted to U.S. (allowed to stay until) - CIC does not use\n",
      "GENDER               Non-immigrant sex\n",
      "INSNUM               INS number\n",
      "AIRLINE              Airline used to arrive in U.S.\n",
      "ADMNUM               Admission Number\n",
      "FLTNO                Flight number of Airline used to arrive in U.S.\n",
      "VISATYPE             Class of admission legally admitting the non-immigrant to temporarily stay in U.S.\n"
     ]
    }
   ],
   "source": [
    "# before going straight into dim_address, parse header file and display nicely\n",
    "with open(HEADER_FILE) as f:\n",
    "    header_file_lines = f.readlines()\n",
    "    \n",
    "comment_lines = [line for line in header_file_lines if line.startswith('/*') and line.endswith('*/\\n')]\n",
    "clpatt = re.compile(r'^/\\*\\s+(?P<code>.+?)\\s+-\\s+(?P<description>.+)\\s+\\*/$')\n",
    "matches = [clpatt.match(cl) for cl in comment_lines]\n",
    "if not all(m is not None for m in matches):\n",
    "    for i, m in enumerate(matches):\n",
    "        if m is None:\n",
    "            print(i)\n",
    "print(f'CODE{\"\":16}', 'DESCRIPTION')\n",
    "for m in matches:\n",
    "    print(f'{m.group(\"code\"):20}', m.group('description'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse header file for dim_adddress values\n",
    "address_lines = header_file_lines[981:1036]\n",
    "patt = re.compile(r\"^\\s*'(?P<code>..)'\\s*=\\s*'(?P<name>.+)'.*$\")\n",
    "matches = [patt.match(line) for line in address_lines]\n",
    "address_codes = {match.group('code'): match.group('name') for match in matches}\n",
    "assert len(address_codes) == len(address_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up queries\n",
    "dim_address_drop = 'DROP TABLE IF EXISTS dim_address;'\n",
    "dim_address_create = 'CREATE TABLE IF NOT EXISTS dim_address (code char(2), name varchar);'\n",
    "dim_address_insert = 'INSERT INTO dim_address (code, name) VALUES (%s, %s);'\n",
    "\n",
    "# execute queries\n",
    "cur.execute(dim_address_drop)\n",
    "cur.execute(dim_address_create)\n",
    "for item in sorted(address_codes.items()):\n",
    "    cur.execute(dim_address_insert, item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dim_port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse header file for dim_adddress values\n",
    "port_lines = header_file_lines[302:962]\n",
    "patt = re.compile(r\"^\\s*'(?P<code>...?)'\\s*=\\s*'(?P<name>.+)'.*$\")\n",
    "matches = [patt.match(line) for line in port_lines]\n",
    "port_codes = {match.group('code'): match.group('name').strip() for match in matches}\n",
    "assert len(port_codes) == len(port_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up queries\n",
    "dim_port_drop = 'DROP TABLE IF EXISTS dim_port;'\n",
    "dim_port_create = 'CREATE TABLE IF NOT EXISTS dim_port (code char(3), name varchar);'\n",
    "dim_port_insert = 'INSERT INTO dim_port (code, name) VALUES (%s, %s);'\n",
    "\n",
    "# execute queries\n",
    "cur.execute(dim_port_drop)\n",
    "cur.execute(dim_port_create)\n",
    "for item in sorted(port_codes.items()):\n",
    "    cur.execute(dim_port_insert, item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dim_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse header file for dim_country values\n",
    "country_lines = header_file_lines[9:298]\n",
    "patt = re.compile(r\"^\\s*(?P<code>\\d+)\\s*=\\s*'(?P<country>.+)'.*$\")\n",
    "matches = [patt.match(line) for line in country_lines]\n",
    "country_codes = {int(match.group('code')): match.group('country') for match in matches}\n",
    "assert len(country_lines) == len(country_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up queries\n",
    "dim_country_drop = 'DROP TABLE IF EXISTS dim_country;'\n",
    "\n",
    "dim_country_create = \"\"\"CREATE TABLE IF NOT EXISTS dim_country\n",
    "(code int PRIMARY KEY, name varchar NOT NULL);\"\"\"\n",
    "\n",
    "dim_country_insert = \"\"\"INSERT INTO dim_country\n",
    "(code, name)\n",
    "VALUES (%s, %s)\n",
    "ON CONFLICT (code) DO NOTHING;\"\"\"\n",
    "\n",
    "# execute queries\n",
    "cur.execute(dim_country_drop)\n",
    "cur.execute(dim_country_create)\n",
    "for item in country_codes.items():\n",
    "    cur.execute(dim_country_insert, item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dim_date\n",
    "\n",
    "I wanted to create `dim_date` because dates are listed in so many different ways in the immigration data. Along with various MDY formats, there's also their preferred format for `arrdate` and `depdate`, which assigns 20454 to January 1, 2016 and increments in the future and decrements in the past.\n",
    "\n",
    "Since the earliest birth year in the data was 1900, I created a row for each day between 1/1/900 and 12/31/2019 inclusive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up queries\n",
    "dim_date_drop = 'DROP TABLE IF EXISTS dim_date;'\n",
    "\n",
    "dim_date_create = \"\"\"CREATE TABLE dim_date\n",
    "(code int PRIMARY KEY, year int NOT NULL, month int NOT NULL,\n",
    " day int NOT NULL, day_of_week INT NOT NULL, ymd_dash char(10) NOT NULL,\n",
    " ymd_nodash char(8) NOT NULL, mdy_nodash char(8) NOT NULL);\n",
    "\"\"\"\n",
    "\n",
    "dim_date_insert = \"\"\"INSERT INTO dim_date\n",
    "(code, year, month, day, day_of_week, ymd_dash, ymd_nodash, mdy_nodash)\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s, %s);\"\"\"\n",
    "\n",
    "# execute queries\n",
    "cur.execute(dim_date_drop)\n",
    "cur.execute(dim_date_create)\n",
    "\n",
    "dt = datetime(2016, 1, 1)\n",
    "end_dt = datetime(2019, 12, 31)\n",
    "one_day = timedelta(days=1)\n",
    "code = 20454\n",
    "\n",
    "while dt <= end_dt:\n",
    "    cur.execute(dim_date_insert, \n",
    "               [code, dt.year, dt.month, dt.day, dt.weekday(), dt.strftime('%Y-%m-%d'),\n",
    "                dt.strftime('%Y%m%d'), dt.strftime('%d%m%Y')]\n",
    "               )\n",
    "    dt = dt + one_day\n",
    "    code += 1\n",
    "    \n",
    "dt = datetime(2015, 12, 31)\n",
    "end_dt = datetime(1900, 1, 1)\n",
    "code = 20453\n",
    "\n",
    "while dt >= end_dt:\n",
    "    cur.execute(dim_date_insert, \n",
    "               [code, dt.year, dt.month, dt.day, dt.weekday(), dt.strftime('%Y-%m-%d'),\n",
    "                dt.strftime('%Y%m%d'), dt.strftime('%d%m%Y')]\n",
    "               )\n",
    "    dt = dt - one_day\n",
    "    code -=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dim_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up queries\n",
    "dim_city_drop = 'DROP TABLE IF EXISTS dim_city;'\n",
    "\n",
    "dim_city_create = \"\"\"CREATE TABLE IF NOT EXISTS dim_city\n",
    "(city varchar, state varchar, median_age numeric, male_pop int, female_pop int, total_pop int, num_vets int,\n",
    "foreign_born int, avg_household_size float, state_code char(2), race varchar, count int);\n",
    "\"\"\"\n",
    "\n",
    "dim_city_insert = \"\"\"INSERT INTO dim_city\n",
    "(city, state, median_age, male_pop, female_pop, total_pop, num_vets, foreign_born, avg_household_size, state_code,\n",
    "race, count)\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s);\"\"\"\n",
    "\n",
    "# execute queries\n",
    "cur.execute(dim_city_drop)\n",
    "cur.execute(dim_city_create)\n",
    "for _, row in city_demo_df.iterrows():\n",
    "    cur.execute(dim_city_insert, [v if pd.notna(v) else None for v in row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dim_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up queries\n",
    "dim_state_drop = 'DROP TABLE IF EXISTS dim_state;'\n",
    "\n",
    "dim_state_create = \"\"\"CREATE TABLE IF NOT EXISTS dim_state\n",
    "(state_code char(2) PRIMARY KEY, male_pop int, female_pop int, total_pop int, foreign_born int);\"\"\"\n",
    "\n",
    "dim_state_insert = \"\"\"INSERT INTO dim_state\n",
    "(state_code, male_pop, female_pop, total_pop, foreign_born)\n",
    "VALUES (%s, %s, %s, %s, %s);\"\"\"\n",
    "\n",
    "# execute queries\n",
    "cur.execute(dim_state_drop)\n",
    "cur.execute(dim_state_create)\n",
    "for _, row in summed_by_state_df.iterrows():\n",
    "    cur.execute(dim_state_insert, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fact_immigration\n",
    "\n",
    "Because `DataFrame.to_sql()` does not work from an ipython interpreter and because we don't want to bring the notebook to a standstill, `fact_immigration` is created and filled by the included script run from the command line\n",
    "\n",
    "`python load_immigration_data.py`\n",
    "\n",
    "Necessary libraries are pandas, pscopg2, tqdm and sqlalchemy. These can be installed via \n",
    "\n",
    "`pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "\n",
    "I performed my data quality checks in the included file `data_quality_checks.sql`. I explain them here.\n",
    "\n",
    "##### row counts for fact_immigration\n",
    "\n",
    "I logged the following row counts when running `load_immigration_data.py`\n",
    "\n",
    "\n",
    "| month\t| df rows |\n",
    "| ----- | -------:|\n",
    "| jan |\t2,847,924 |\n",
    "| feb |\t2,570,543 |\n",
    "| mar |\t3,157,072 |\n",
    "| apr |\t3,096,313 |\n",
    "| may |\t3,444,249 |\n",
    "| jun |\t3,574,989 |\n",
    "| jul |\t4,265,031 |\n",
    "| aug |\t4,103,570 | \n",
    "| sep |\t3,733,786 |\n",
    "| oct |\t3,649,136 |\n",
    "| nov |\t2,914,926 |\n",
    "| dec |\t3,432,990 |\n",
    " \n",
    "This allowed me to run the following query and compare the results\n",
    "\n",
    "```sql\n",
    "select count(distinct immigration_id), i94mon\n",
    "from fact_immigration\n",
    "group by i94mon\n",
    "order by i94mon;\n",
    "```\n",
    "\n",
    "##### Only 2016 data\n",
    "\n",
    "I also assured myself there was no data outside of 2016\n",
    "\n",
    "```sql\n",
    "select count(1) from fact_immigration where i94yr is null or i94yr <> 2016;\n",
    "```\n",
    "\n",
    "##### Explore `cicid`\n",
    "\n",
    "I noticed that `cicid` was duplicated across each month's data, and I wondered if that was actually supposed to be a cross-file primary key and so I might have duplicates by using my own primary key. I was able to verify they are not duplicates using this set of queries\n",
    "\n",
    "```sql\n",
    "select cicid, count(cicid) from fact_immigration group by cicid order by count desc;\n",
    "\n",
    "-- over 500 cicids appear 12 times...let's look at a handful to see if they are duplicates\n",
    "select * from fact_immigration\n",
    "where cicid in (5454856, 3334634, 4087143, 395680)\n",
    "order by cicid; -- those are very clearly not duplicates\n",
    "```\n",
    "\n",
    "##### Fixing `i94bir` and `biryear`\n",
    "\n",
    "As explained in Step 2 above, I discovered ages and birth years that did not makes sense in this data, and so I fixed them with these queries.\n",
    "\n",
    "```sql\n",
    "UPDATE fact_immigration\n",
    "SET i94bir = NULL\n",
    "WHERE i94bir < 0 or i94bir > 120;\n",
    "\n",
    "UPDATE fact_immigration\n",
    "SET biryear = NULL\n",
    "WHERE biryear < 1900 or biryear > 2016;\n",
    "```\n",
    "\n",
    "##### count\n",
    "\n",
    "I make sure the `count` field is always 1 with\n",
    "\n",
    "```sql\n",
    "select count(count) N, count\n",
    "from fact_immigration\n",
    "group by count\n",
    "order by N desc;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "##### fact_immigration\n",
    "\n",
    "The grain is an immigration event\n",
    "\n",
    "* immigration_id: primary key\n",
    "* cicid: unique key within a month\n",
    "* i94yr: 4 digit year, always 2016\n",
    "* i94mon: numeric month, 1-12\n",
    "* i94cit: immigrant's country of citizenship; foreign key to `dim_country`\n",
    "* i94res: immigrant's country of residence outside US; foreign key to `dim_country`\n",
    "* i94port: port of entry; foreign key to `dim_port`\n",
    "* arrdate: arrival date of immigrant where 20454 == 1/1/2016\n",
    "* i94mode: mode of arrival; foreign key to `dim_arrival_mode`\n",
    "* i94addr: address (usually state) of immigrant in US; foreign key to `dim_address`\n",
    "* depdate: departure date of immigrant where 20454 == 1/1/2016\n",
    "* i94bir: immigrant's age in years\n",
    "* i94visa: foreign key to `dim_visa_type`\n",
    "* count: used for summary statistics; always 1 (for easy adding)\n",
    "* dtadfile: dates in the format YYYYMMDD\n",
    "* visapost: three-letter codes corresponding to where visa was issued\n",
    "* occup: occupation in US of immigration. Mostly STU for student, also many OTH for other\n",
    "* entdepa: one-letter arrival code\n",
    "* entdepd: one-letter departure code\n",
    "* entdepu: one-letter update code\n",
    "* matflag: M if the arrival and departure records match\n",
    "* biryear: four-digit year of birth\n",
    "* dtaddto: MMDDYYYY date field for when the immigrant is admitted until\n",
    "* gender: mostly M and F, but some X and U as well\n",
    "* insnum: Immigration and Naturalization Services number; many re-used\n",
    "* airline: Airline of entry for immigrant\n",
    "* admnum: admission number; many re-used, but not as much as insnum\n",
    "* fltno: flight number of immigrant\n",
    "* visatype: short visa codes like WT, B2, WB, etc.\n",
    "\n",
    "##### dim_city\n",
    "\n",
    "Provides population statistics on cities in the US. Grain is city/state/race.\n",
    "\n",
    "* city: city's name\n",
    "* state: state city is in\n",
    "* median_age: median age of city\n",
    "* male_pop: number of men in the city\n",
    "* female_pop: number of women in the city\n",
    "* total_pop: number of people in the city\n",
    "* num_vets: number of veterans in the city\n",
    "* foreign_born: number of foreign-born people in the city\n",
    "* avg_household_size: average household size\n",
    "* state_code: two-letter code for state\n",
    "* race: White, Hispanic or Latino, Asian, Black or African-American, or American Indian and Alaska Native\n",
    "* count: number of people of that race in the city\n",
    "\n",
    "##### dim_state\n",
    "\n",
    "Aggregated statistics from dim_city by state\n",
    "\n",
    "* state_code: two-letter code for state\n",
    "* male_pop: number of men in the state\n",
    "* female_pop: number of women in the state\n",
    "* total_pop: number of people in the state\n",
    "* foreign_born: number of foreign-born people in the state\n",
    "\n",
    "##### dim_country\n",
    "\n",
    "A list of countries and their codes that appear in `fact_immigration.i94cit` and `fact_immigration.i94res`\n",
    "\n",
    "* code: a numbered code\n",
    "* name: usually a name of a country. There are many that start with `INVALID:` as well as several different `No Country Code([code])` values\n",
    "\n",
    "##### dim_address\n",
    "\n",
    "A list of the states (usually) where immigrants list their address\n",
    "\n",
    "* code: mostly two-letter codes for states. There's DC, GU (Guam), and 99 (All Other Codes) as well\n",
    "* name: name of state, region, etc.\n",
    "\n",
    "##### dim_port\n",
    "\n",
    "A list of the ports of arrival\n",
    "\n",
    "* code: a short code\n",
    "* name: the name of the port; there are some `No PORT Code ([code])` values too\n",
    "\n",
    "##### dim_date\n",
    "\n",
    "A list of dates in different formats\n",
    "\n",
    "* code: the CIC code for date where 20454 is 1/1/2016\n",
    "* year: four-digit year\n",
    "* month: month; 1-12\n",
    "* day: day; 1-31\n",
    "* day_of_week: 0 for Monday, 1 for Tuesday, ..., 7 for Sunday\n",
    "* ymd_dash: date formatted as YYYY-MM-DD\n",
    "* ymd_nodash: date formatted as YYYYMMDD\n",
    "* mdy_noash: date formatted as MMDDYYYY\n",
    "\n",
    "##### dim_arrival_mode\n",
    "\n",
    "How immigrants arrived. Foreign key to `fact_immigration.i94mode`\n",
    "\n",
    "* code: 1, 2, 3, or 9\n",
    "* mode: Air, Sea, Land, or Not reported, respectively\n",
    "\n",
    "##### dim_visa_type\n",
    "\n",
    "The type of visa the immigrant is coming in on. Foreigy key to `fact_immigration.i94visa`\n",
    "\n",
    "* code: 1, 2, or 3\n",
    "* visa_type: Business, Pleasure, or Student, respectively\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Complete Project Write Up\n",
    "\n",
    "#### Rationale\n",
    "\n",
    "##### Tools and technologies\n",
    "\n",
    "I chose Python and Pandas because it can easily read all the data formats provided and then easily get them into a relational database.\n",
    "\n",
    "The data was structured and formatted well enough to make using a SQL relational database a good fit, as it would provide a nice underpinning for an easily queryable star schema. I chose Postgres because it is fast and robust.\n",
    "\n",
    "##### Data Model\n",
    "\n",
    "I chose to augment the immigration data with the city and state dimensions because the `i94addr` column indicates the state where the entrants have an address. This allowed me to see what states immigrants were heading to, along with whether, say, those states have a higher-percentage of foreign-born residents already, whether men are heading to states with a higher-percentage of women, etc.\n",
    "\n",
    "#### Updates\n",
    "\n",
    "* `fact_immigration` needs to be updated monthly when each new dataset is available\n",
    "* with a header file, all `dim_` tables source from the immigration data can be dropped and recreated completely as above\n",
    "* `dim_city` and `dim_state` don't have a time component now. If new data is available in the future they should be updated at that time, ideally with date columns, at least by year\n",
    "* `dim_date` should be kept up to date. To be safe all dates between through 12/31/2099 should be added\n",
    "\n",
    "#### If things were different...\n",
    "\n",
    "If the data were increased by 100 times, what is now a six-hour load would become an untenable 600-hour load. To avoid this, I would instead convert the `fact_immigration` data to a format readable by Redshift Spectrum and land the data to S3, partitioned by date, and create an external schema so Redshift Spectrum could read it in a schema-on-read fashion.\n",
    "\n",
    "Under this scenario, I could also ensure that the data was ready to populate a dashboard by seven AM every day as well. I have to assume I could get the daily data for `fact_immigration` and, when it is avaialble, I could have an Airflow DAG using a `S3Sensor` that kicked off upon its arrival and then proceeded to parse the data, land it in its date-partitioned location in S3, in which case it would be ready for Redshift Spectrum to read immediately.\n",
    "\n",
    "There should be no problem with 100 or so people accessing this data. However, the date-partitioned nature of the solution proposed above would also help in this case. If access by multiple users continues to be a problem you can mitigate that by having the data replicate to different nodes used by different users. If your users are located around the world, a replication node near each group of people would be best."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
